{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/fanbyprinciple/using-gpt3-to-produce-jokes?scriptVersionId=84126476\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Lets first try using gpt3 in kaggle","metadata":{}},{"cell_type":"markdown","source":"Adapted from https://github.com/shreyashankar/gpt3-sandbox","metadata":{}},{"cell_type":"markdown","source":"### installing necessary libraries","metadata":{}},{"cell_type":"code","source":"!pip install openai","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:19:25.80616Z","iopub.execute_input":"2022-01-01T10:19:25.806659Z","iopub.status.idle":"2022-01-01T10:19:39.403532Z","shell.execute_reply.started":"2022-01-01T10:19:25.806552Z","shell.execute_reply":"2022-01-01T10:19:39.402586Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting openai\n  Downloading openai-0.11.5.tar.gz (466 kB)\n     |████████████████████████████████| 466 kB 903 kB/s            \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.7/site-packages (from openai) (2.25.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from openai) (4.62.3)\nRequirement already satisfied: pandas>=1.2.3 in /opt/conda/lib/python3.7/site-packages (from openai) (1.3.4)\nCollecting pandas-stubs>=1.1.0.11\n  Downloading pandas_stubs-1.2.0.39-py3-none-any.whl (161 kB)\n     |████████████████████████████████| 161 kB 11.2 MB/s            \n\u001b[?25hCollecting openpyxl>=3.0.7\n  Downloading openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n     |████████████████████████████████| 242 kB 11.3 MB/s            \n\u001b[?25hCollecting et-xmlfile\n  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.3->openai) (2.8.0)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.3->openai) (2021.3)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.2.3->openai) (1.19.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from pandas-stubs>=1.1.0.11->openai) (3.10.0.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (1.26.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.20->openai) (2.10)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.16.0)\nBuilding wheels for collected packages: openai\n  Building wheel for openai (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for openai: filename=openai-0.11.5-py3-none-any.whl size=161463 sha256=bcaf318a5fe0990b9bbd0119e565e9ee69f15d16aa97a94262350971e52743f2\n  Stored in directory: /root/.cache/pip/wheels/5a/52/66/06c234d9748d81b9622d0b9765c3bd10e99d059ed93f13b0b3\nSuccessfully built openai\nInstalling collected packages: et-xmlfile, pandas-stubs, openpyxl, openai\nSuccessfully installed et-xmlfile-1.1.0 openai-0.11.5 openpyxl-3.0.9 pandas-stubs-1.2.0.39\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### This is a helper function for easy input and access to openai's api","metadata":{}},{"cell_type":"code","source":"\n\"\"\"Creates the Example and GPT classes for a user to interface with the OpenAI\nAPI.\"\"\"\n\nimport openai\nimport uuid\n\n\ndef set_openai_key(key):\n    \"\"\"Sets OpenAI key.\"\"\"\n    openai.api_key = key\n\n\nclass Example:\n    \"\"\"Stores an input, output pair and formats it to prime the model.\"\"\"\n    def __init__(self, inp, out):\n        self.input = inp\n        self.output = out\n        self.id = uuid.uuid4().hex\n\n    def get_input(self):\n        \"\"\"Returns the input of the example.\"\"\"\n        return self.input\n\n    def get_output(self):\n        \"\"\"Returns the intended output of the example.\"\"\"\n        return self.output\n\n    def get_id(self):\n        \"\"\"Returns the unique ID of the example.\"\"\"\n        return self.id\n\n    def as_dict(self):\n        return {\n            \"input\": self.get_input(),\n            \"output\": self.get_output(),\n            \"id\": self.get_id(),\n        }\n\n\nclass GPT:\n    \"\"\"The main class for a user to interface with the OpenAI API.\n    A user can add examples and set parameters of the API request.\n    \"\"\"\n    def __init__(self,\n                 engine='davinci',\n                 temperature=0.5,\n                 max_tokens=100,\n                 input_prefix=\"input: \",\n                 input_suffix=\"\\n\",\n                 output_prefix=\"output: \",\n                 output_suffix=\"\\n\\n\",\n                 append_output_prefix_to_query=False):\n        self.examples = {}\n        self.engine = engine\n        self.temperature = temperature\n        self.max_tokens = max_tokens\n        self.input_prefix = input_prefix\n        self.input_suffix = input_suffix\n        self.output_prefix = output_prefix\n        self.output_suffix = output_suffix\n        self.append_output_prefix_to_query = append_output_prefix_to_query\n        self.stop = (output_suffix + input_prefix).strip()\n\n    def add_example(self, ex):\n        \"\"\"Adds an example to the object.\n        Example must be an instance of the Example class.\n        \"\"\"\n        assert isinstance(ex, Example), \"Please create an Example object.\"\n        self.examples[ex.get_id()] = ex\n\n    def delete_example(self, id):\n        \"\"\"Delete example with the specific id.\"\"\"\n        if id in self.examples:\n            del self.examples[id]\n\n    def get_example(self, id):\n        \"\"\"Get a single example.\"\"\"\n        return self.examples.get(id, None)\n\n    def get_all_examples(self):\n        \"\"\"Returns all examples as a list of dicts.\"\"\"\n        return {k: v.as_dict() for k, v in self.examples.items()}\n\n    def get_prime_text(self):\n        \"\"\"Formats all examples to prime the model.\"\"\"\n        return \"\".join(\n            [self.format_example(ex) for ex in self.examples.values()])\n\n    def get_engine(self):\n        \"\"\"Returns the engine specified for the API.\"\"\"\n        return self.engine\n\n    def get_temperature(self):\n        \"\"\"Returns the temperature specified for the API.\"\"\"\n        return self.temperature\n\n    def get_max_tokens(self):\n        \"\"\"Returns the max tokens specified for the API.\"\"\"\n        return self.max_tokens\n\n    def craft_query(self, prompt):\n        \"\"\"Creates the query for the API request.\"\"\"\n        q = self.get_prime_text(\n        ) + self.input_prefix + prompt + self.input_suffix\n        if self.append_output_prefix_to_query:\n            q = q + self.output_prefix\n\n        return q\n\n    def submit_request(self, prompt):\n        \"\"\"Calls the OpenAI API with the specified parameters.\"\"\"\n        response = openai.Completion.create(engine=self.get_engine(),\n                                            prompt=self.craft_query(prompt),\n                                            max_tokens=self.get_max_tokens(),\n                                            temperature=self.get_temperature(),\n                                            top_p=1,\n                                            n=1,\n                                            stream=False,\n                                            stop=self.stop)\n        return response\n\n    def get_top_reply(self, prompt):\n        \"\"\"Obtains the best result as returned by the API.\"\"\"\n        response = self.submit_request(prompt)\n        return response['choices'][0]['text']\n\n    def format_example(self, ex):\n        \"\"\"Formats the input, output pair.\"\"\"\n        return self.input_prefix + ex.get_input(\n        ) + self.input_suffix + self.output_prefix + ex.get_output(\n        ) + self.output_suffix\n","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:19:39.405578Z","iopub.execute_input":"2022-01-01T10:19:39.405844Z","iopub.status.idle":"2022-01-01T10:19:39.441386Z","shell.execute_reply.started":"2022-01-01T10:19:39.405812Z","shell.execute_reply":"2022-01-01T10:19:39.440634Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nimport openai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-01T10:19:39.443337Z","iopub.execute_input":"2022-01-01T10:19:39.443529Z","iopub.status.idle":"2022-01-01T10:19:39.44781Z","shell.execute_reply.started":"2022-01-01T10:19:39.443504Z","shell.execute_reply":"2022-01-01T10:19:39.447283Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### input your openai secret key here\n\nYou can get your openai keys from here: https://beta.openai.com/account/api-keys","metadata":{}},{"cell_type":"code","source":"import getpass\ncity = getpass.getpass('Input your open ai service key:')","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:19:39.449441Z","iopub.execute_input":"2022-01-01T10:19:39.449674Z","iopub.status.idle":"2022-01-01T10:20:35.65365Z","shell.execute_reply.started":"2022-01-01T10:19:39.449641Z","shell.execute_reply":"2022-01-01T10:20:35.652488Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdin","text":"Input your open ai service key: ···················································\n"}]},{"cell_type":"code","source":"openai.api_key = city","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:35.654981Z","iopub.execute_input":"2022-01-01T10:20:35.655563Z","iopub.status.idle":"2022-01-01T10:20:35.660017Z","shell.execute_reply.started":"2022-01-01T10:20:35.655511Z","shell.execute_reply":"2022-01-01T10:20:35.659018Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Lets try some well documented use cases of openai","metadata":{}},{"cell_type":"markdown","source":"### Application 1 - Directly use as chatbot Use-case","metadata":{}},{"cell_type":"code","source":"\ngpt1 = GPT(engine=\"davinci\", temperature=0.2, max_tokens=100)\nprompt1 = \"How to learn data science?\"\noutput1 = gpt1.submit_request(prompt1)\noutput1.choices[0].text","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:35.661607Z","iopub.execute_input":"2022-01-01T10:20:35.661925Z","iopub.status.idle":"2022-01-01T10:20:39.542209Z","shell.execute_reply.started":"2022-01-01T10:20:35.661884Z","shell.execute_reply":"2022-01-01T10:20:39.541482Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'\\nI’ve been a data scientist for more than a decade. I’ve worked in academia, industry, and startups. I’ve built models that predict the future and ones that optimize the present. I’ve worked with data that’s been collected for decades and data that was collected yesterday. I’ve worked with data that’s been collected by thousands of people and data that was collected by one person. I’ve worked with data'"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Application 2 - LaTex Use-case - Text to Equation\ngpt2 = GPT(engine=\"davinci\", temperature=0.5, max_tokens=100)\n# Directly use as chatbot\ngpt2.add_example(Example('Two plus two equals four', '2 + 2 = 4'))\ngpt2.add_example(Example('The integral from zero to infinity', '\\\\int_0^{\\\\infty}'))\ngpt2.add_example(Example('The gradient of x squared plus two times x with respect to x', '\\\\nabla_x x^2 + 2x'))\ngpt2.add_example(Example('The log of two times x', '\\\\log{2x}'))\ngpt2.add_example(Example('x squared plus y squared plus equals z squared', 'x^2 + y^2 = z^2'))","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:39.543235Z","iopub.execute_input":"2022-01-01T10:20:39.543487Z","iopub.status.idle":"2022-01-01T10:20:39.550061Z","shell.execute_reply.started":"2022-01-01T10:20:39.543459Z","shell.execute_reply":"2022-01-01T10:20:39.549201Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"prompt2 = \"x squared plus 2 times x\"\noutput2 = gpt2.submit_request(prompt2)\noutput2.choices[0].text","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:39.551073Z","iopub.execute_input":"2022-01-01T10:20:39.55177Z","iopub.status.idle":"2022-01-01T10:20:40.573606Z","shell.execute_reply.started":"2022-01-01T10:20:39.551737Z","shell.execute_reply":"2022-01-01T10:20:40.572755Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'output: x^2 + 2x\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Application 3 - Translator Use-case\ngpt3 = GPT(engine=\"davinci\", temperature=0.5, max_tokens=100)\ngpt3.add_example(Example('What is your name?', 'quel est votre nom?'))\ngpt3.add_example(Example('What are you doing?', 'Que faites-vous?'))\ngpt3.add_example(Example('How are you?', 'Comment allez-vous?'))","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:40.574939Z","iopub.execute_input":"2022-01-01T10:20:40.575238Z","iopub.status.idle":"2022-01-01T10:20:40.581126Z","shell.execute_reply.started":"2022-01-01T10:20:40.575196Z","shell.execute_reply":"2022-01-01T10:20:40.58025Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"prompt3 = \"where are you?\"\noutput3 = gpt3.submit_request(prompt3)\noutput3.choices[0].text","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:40.584336Z","iopub.execute_input":"2022-01-01T10:20:40.584998Z","iopub.status.idle":"2022-01-01T10:20:42.161464Z","shell.execute_reply.started":"2022-01-01T10:20:40.584963Z","shell.execute_reply":"2022-01-01T10:20:42.160757Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'output: Où êtes-vous?\\n\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Once that is done, lets try and train this model for producing jokes","metadata":{}},{"cell_type":"code","source":"gpt4 = GPT(engine=\"davinci\", temperature=0.5, max_tokens=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.162518Z","iopub.execute_input":"2022-01-01T10:20:42.162806Z","iopub.status.idle":"2022-01-01T10:20:42.166969Z","shell.execute_reply.started":"2022-01-01T10:20:42.162775Z","shell.execute_reply":"2022-01-01T10:20:42.166461Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.168387Z","iopub.execute_input":"2022-01-01T10:20:42.168934Z","iopub.status.idle":"2022-01-01T10:20:42.178378Z","shell.execute_reply.started":"2022-01-01T10:20:42.168893Z","shell.execute_reply":"2022-01-01T10:20:42.177789Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"jokes = pd.read_csv('../input/qa-jokes/jokes.csv')\njokes.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.179375Z","iopub.execute_input":"2022-01-01T10:20:42.180021Z","iopub.status.idle":"2022-01-01T10:20:42.353358Z","shell.execute_reply.started":"2022-01-01T10:20:42.179988Z","shell.execute_reply":"2022-01-01T10:20:42.352543Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   ID                                           Question  \\\n0   1  Did you hear about the Native American man tha...   \n1   2       What's the best anti diarrheal prescription?   \n2   3  What do you call a person who is outside a doo...   \n3   4  Which Star Trek character is a member of the m...   \n4   5  What's the difference between a bullet and a h...   \n\n                                Answer  \n0  He nearly drown in his own tea pee.  \n1                     Mycheexarphlexin  \n2                                 Matt  \n3                   Jean-Luc Pickacard  \n4        A bullet doesn't miss Harambe  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Did you hear about the Native American man tha...</td>\n      <td>He nearly drown in his own tea pee.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>What's the best anti diarrheal prescription?</td>\n      <td>Mycheexarphlexin</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>What do you call a person who is outside a doo...</td>\n      <td>Matt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Which Star Trek character is a member of the m...</td>\n      <td>Jean-Luc Pickacard</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>What's the difference between a bullet and a h...</td>\n      <td>A bullet doesn't miss Harambe</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(jokes)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.354765Z","iopub.execute_input":"2022-01-01T10:20:42.35528Z","iopub.status.idle":"2022-01-01T10:20:42.361225Z","shell.execute_reply.started":"2022-01-01T10:20:42.355233Z","shell.execute_reply":"2022-01-01T10:20:42.360372Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"38269"},"metadata":{}}]},{"cell_type":"code","source":"# just checking\njokes['Question'][0]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.362705Z","iopub.execute_input":"2022-01-01T10:20:42.36309Z","iopub.status.idle":"2022-01-01T10:20:42.37578Z","shell.execute_reply.started":"2022-01-01T10:20:42.363059Z","shell.execute_reply":"2022-01-01T10:20:42.3749Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'Did you hear about the Native American man that drank 200 cups of tea?'"},"metadata":{}}]},{"cell_type":"markdown","source":"Are you ready to prompt it?","metadata":{}},{"cell_type":"code","source":"import random\nprint(\"Prompts selected: \")\nfor i in range(5):\n    lucky_number =  random.randrange(0,38269,1)\n    ques = jokes['Question'][lucky_number]\n    ans = jokes['Answer'][lucky_number]\n    print(i, \": \", ques , \" : \", ans)\n    gpt4.add_example(Example(ques,ans))","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.377049Z","iopub.execute_input":"2022-01-01T10:20:42.377245Z","iopub.status.idle":"2022-01-01T10:20:42.390947Z","shell.execute_reply.started":"2022-01-01T10:20:42.377221Z","shell.execute_reply":"2022-01-01T10:20:42.389667Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Prompts selected: \n0 :  Why did the plane crash into the mountain?  :  Because the pilot was a loaf of bread. \n1 :  How were you conceived?  :  Daddy came on his shoe and kicked mommy in the ass. Or Daddy came on the wall and mommy ran against the wall.\n2 :  Who makes the best milkshakes ever?  :  Michael J. Fox.\n3 :  What did Hitler say when he ran out of soap?  :  Man, I'd kill 5 million Jews for some soap right about now. \n4 :  Why don't Muslims eat pork?  :  They don't support cannibalism.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can get individual examples like this:","metadata":{}},{"cell_type":"code","source":"gpt4.get_example(1)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.392769Z","iopub.execute_input":"2022-01-01T10:20:42.39331Z","iopub.status.idle":"2022-01-01T10:20:42.398937Z","shell.execute_reply.started":"2022-01-01T10:20:42.393265Z","shell.execute_reply":"2022-01-01T10:20:42.398158Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"prompt = \"why did chicken cross the road?\"\noutput = gpt4.submit_request(prompt)\noutput.choices[0].text","metadata":{"execution":{"iopub.status.busy":"2022-01-01T10:20:42.400442Z","iopub.execute_input":"2022-01-01T10:20:42.400754Z","iopub.status.idle":"2022-01-01T10:20:44.547945Z","shell.execute_reply.started":"2022-01-01T10:20:42.400714Z","shell.execute_reply":"2022-01-01T10:20:44.547112Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"\"output: I don't know, but if you give it to me, I'll cross it for you.\\n\\n\""},"metadata":{}}]},{"cell_type":"markdown","source":"Hmm could work on the technique but it does have a sense of humour!","metadata":{}}]}